# -*- coding: utf-8 -*-
"""Sentiment Analysis _Supercars.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RwGFnM-FzNzopDX1WvkOexIkbICWnvXC

## **Mount Google Drive in Colab**
"""

from google.colab import drive
drive.mount('/content/drive')

# cd '/content/drive/My Drive/Colab Notebooks/MKTG6010/Group'

"""## **Install Packages and Load Dataset**"""

!pip install nltk
!pip install sklearn
!pip install wordcloud
import pandas as pd
import numpy as np
!pip install num2words
!pip install PyLDAVis
import pyLDAvis.sklearn
!pip install dataprep
import statsmodels.api as sm
import matplotlib.pyplot as plt

data_all = pd.read_csv('/content/drive/My Drive/Data/tweets_sport-1.csv')
# data_all = pd.read_csv('/content/drive/MyDrive/tweets_sport-1.csv')

text=data_all['text']

tv_data = pd.read_csv('/content/drive/My Drive/Data/tv_rating_new-4.csv')

"""## Q1: What is the most popular topic for each event?"""

import warnings
warnings.filterwarnings('ignore')

data = data_all

text = data['text']

text = text.map(lambda text: text.lower())
text = text.values.tolist()

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer

tokens = []
for sent in text:
    temp = [WordNetLemmatizer().lemmatize(word) for word in sent.split(' ')]
    tokens.append(temp)
    
data['tokens'] = tokens

from sklearn.feature_extraction import text
my_additional_stop_words = ['bathurst','bathurst1000','v8sc',
                            'v8supercars','supercar','supercars',
                           'v8supercar','adelaide','http','tas',
                           'tcm','darwin','coateshire','sandown',
                           'sandown500','clipsal','clipsal500',
                          'sydney', 'townsville','t','rt','co']
stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)
stopwords = list(stop_words)

"""### LDA"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from seaborn import heatmap
import matplotlib.pyplot as plt

def LDA(data):
    NUM_TOPICS = 10
    text_train = list(data['tokens'].apply(lambda x: ' '.join(x)))

    vectorizer = CountVectorizer(min_df=5, max_df=0.9,
                                 stop_words=stopwords, lowercase=True,
                                 token_pattern='[a-zA-Z\-][a-zA-Z\-]{2,}')

    data_vectorized = vectorizer.fit_transform(text_train)

    lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, random_state=258, learning_method='online')
    lda_Z = lda_model.fit_transform(data_vectorized)
    return vectorizer, data_vectorized, lda_model, lda_Z


def topics_to_df(model, vectorizer, title, top_n=10):
    words = []
    coefs = []
    for idx, topic in enumerate(model.components_):
        for i in topic.argsort()[:-top_n - 1:-1]:
            words.append(vectorizer.get_feature_names_out()[i])
        coefs.append([topic[i] for i in topic.argsort()[:-top_n - 1:-1]])
    coefs = pd.DataFrame(coefs)
    x_ticks = ['Word {}'.format(i + 1) for i in range(top_n)]
    y_ticks = ['Topic {}'.format(i + 1) for i in range(len(coefs))]
    ax = heatmap(coefs, annot=True, fmt='.1f',
                 xticklabels=x_ticks, yticklabels=y_ticks,
                 cmap="coolwarm", cbar=False)
    ax.set_title(title)
    for t in ax.texts:
        text = t.get_text()
        t.set_text('{}\n'.format(words[0]) + text)
        t.set_fontsize(18)
        words.pop(0)
    return ax

"""### Topic Map"""

import pyLDAvis.sklearn
def topic_map(lda_model, data_vectorized, vectorizer):
    pyLDAvis.enable_notebook()
    panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne', sort_topics = False)
    return panel

"""### Entire Dataset"""

vectorizer, data_vectorized, lda_model, lda_Z = LDA(data)

fig = plt.figure(figsize=(25, 10), facecolor='w', edgecolor='k')
ax = topics_to_df(lda_model, vectorizer, 'Overall Topic Heatmap')

panel = topic_map(lda_model, data_vectorized, vectorizer)
panel

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from IPython.core.pylabtools import figsize

df_tweets = list(data['tokens'].apply(lambda x: ' '.join(x)))
tweets = ''.join(df_tweets)
wordcloud = WordCloud(stopwords=stop_words,background_color="white", colormap='tab10', max_words=300).generate(tweets)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Events"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(60, 80), dpi=300, facecolor='w', edgecolor='k')
panels = []

for i in range(1, 15):
    df = data[data['event_number']==i]
    vectorizer, data_vectorized, lda_model, lda_Z = LDA(df)
    plt.subplot(7, 2, i)
    ax = topics_to_df(lda_model, vectorizer, 'Topic Heatmap for Event {}'.format(i))
    
    panels.append(topic_map(lda_model, data_vectorized, vectorizer))

panels[4]

"""##Q2ï¼š Is there correlation between popularity of each event and sentiment score?

"""

text=data_all['text']

#Change the text to lower case
text = text.map(lambda text: text.lower())
#Convert data from pandas format to list values  
text=text.values.tolist()

#print the first 3 tweets
print(text[:3])

import nltk
nltk.download('wordnet')

# Conduct lemmatization for the words in the text
from nltk.stem import WordNetLemmatizer

tokens=[]
for sent in text:
    temp=[WordNetLemmatizer().lemmatize(word) for word in sent.split(" ")]
    tokens.append(temp)

# Customized the stopwords

from sklearn.feature_extraction import text 
my_additional_stop_words = ["bathurst","adelaide","http","tas","tcm","darwin","coateshire","sandown","clipsal","sydney", "townsville"]
stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)
stopwords = list(stop_words)

"""### To get the variable of Sentiment score and absolute value of sentiment score


"""

x = lda_model.transform(data_vectorized)
print(x[3])

print(data_all['text'][3])

topics=pd.DataFrame(x)
topics['tweet']=data_all['text']
topics['month']=data_all['month']
topics['day']=data_all['day']
topics['time']=data_all['time']
topics['event_number']=data_all['event_number']
topics['weekday']=data_all['weekday']
# pd.DataFrame(topics).to_csv('Data/tweets_by_topics_all_new.csv',index=False)

topics

import nltk
nltk.download('vader_lexicon')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # import `SentimentIntensityAnalyzer` and load a model
# from nltk.sentiment.vader import SentimentIntensityAnalyzer
# sentiment = SentimentIntensityAnalyzer()

# Using the model to process each tweet and call `compound` as polarity score

scores=[]
for tex in topics['tweet']:
    sentimentResults = sentiment.polarity_scores(tex)
    score = sentimentResults["compound"]
    scores.append(score)

# scorep=[]
# for tex in topics['tweet']:
#     sentimentResults = sentiment.polarity_scores(tex)
#     score1 = sentimentResults["pos"]
#     scorep.append(score1)

# Show the score of index 1 
scores
score_ab=[]
for i in scores:
  if i>=0:
    ab=i
    score_ab.append(ab)
  else:
    ab=-i
    score_ab.append(ab)
print(score_ab)

# Get the sentiment scores weighted by the topic relevance probability
### Create a new variable named 'topic_senti', and the values are sentiment score * topic relevance probability

# # Topic 1
# topics['topic1_senti'] = topics['topic8'] * scores
topics['senti']=scores
topics['senti_ab']=score_ab

"""###Group data to get the average sentiment score (as well as its absoulute value) and the sum of tv viewers and tweets for each event

"""

topics

group= topics.groupby([pd.Grouper('event_number')]).agg(senti_mean=('senti', 'mean'),senti_ab_mean=('senti_ab', 'mean'),number_of_tweet=('tweet', 'count'))

group

# reset_index to break the pivot table to normal table
group = group.reset_index()
group.head()

# topics.to_csv('Data/topics.csv')

tv_data

group_tv = tv_data.groupby([pd.Grouper('event_number')]).agg(number_of_tv_viewer=('tvviewers', 'sum'))

group_tv

# reset_index to break the pivot table to normal table
group_tv = group_tv.reset_index()
group_tv.head()

merged_data_ = pd.merge(group_tv, group, how='left',
                       left_on=['event_number'],
                       right_on=['event_number'])
# drop the null value
merged_data_ = merged_data_.dropna()
len(merged_data_)

merged_data_.head()

"""###Calculate the correlation index

"""

coefficient=merged_data_[['number_of_tweet','number_of_tv_viewer','senti_ab_mean','senti_mean']].corr(method='pearson')
coefficient

"""## **Q3: How does eWOM communications impact TV rating and online engagements**

### Supercar Tweets Data
"""

text=data_all['text']

#Change the text to lower case
data = text.map(lambda text: text.lower())

#Convert data from pandas format to list values  
text=text.values.tolist()

#print the first 3 tweets
print(text[:3])

print(type(text))

data_all.describe()

tweets_data = pd.read_csv('/content/drive/My Drive/Data/tweets_sport-1.csv', parse_dates=['time'])
tweets_data.head(15)

#group tweet data to every 15 mins
group_tweet_data = tweets_data.groupby([pd.Grouper('event_number'),pd.Grouper('weekday'),pd.Grouper(key='time', freq='15min')]).agg(number_of_tweet=('text', 'count'), 
                                                                              number_of_player=('player_dummy','sum'), 
                                                                              number_of_team=('team_dummy','sum'),
                                                                              total_retweets=('retweets','sum'), 
                                                                              total_likes = ('likes','sum'), 
                                                                              total_comments = ('comments','sum'))

# reset_index to break the pivot table to normal table
group_tweet_data = group_tweet_data.reset_index()
group_tweet_data.head()

"""### TV Rating Data"""

tv_data.head()

#change city to dummy 
dummy_city = pd.get_dummies(tv_data["city"],drop_first=True)

#merge dummy into dataset
tv_data = tv_data.merge(dummy_city,left_index=True, right_index=True)
tv_data

"""### Merge TV Rating Data and Supercar Tweets Data"""

# create a new column 'start_time' to match the tv_rating: time + 15min
import datetime as dt
group_tweet_data['start_time'] = group_tweet_data['time'] + dt.timedelta(minutes=15)

# change the timestamp to string
group_tweet_data['time'] = group_tweet_data['time'].apply(lambda x: x.strftime('%H:%M'))
group_tweet_data['start_time'] = group_tweet_data['start_time'].apply(lambda x: x.strftime('%H:%M'))

merged_data = pd.merge(tv_data, group_tweet_data, how='left',
                       left_on=['start_time','event_number','weekday'],
                       right_on=['start_time','event_number','weekday'])

# drop the null value
merged_data = merged_data.dropna()
len(merged_data)

#change weekday to dummy 
dummy_day = pd.get_dummies(tv_data["weekday"],drop_first=True)

#merge dummy into dataset
merged_data = merged_data.merge(dummy_day,left_index=True, right_index=True)
merged_data

merged_data.describe()

"""### **Q3(1): How does eWOM communications in different locations impact TV viewers?** """

#group by city viewership
group_merged_data = merged_data.groupby('city').agg(city=('city','count'), tvviewers=('tvviewers','sum'), number_of_player=('number_of_player','sum'),number_of_team=('number_of_team','sum'), total_retweets=('total_retweets','sum'),total_likes = ('total_likes','sum'), total_comments = ('total_comments','sum'))
                                                                                                                                                                                                                                        
group_merged_data

"""Based on the table, it can be seen that each cities are represented somewhat equally with an exception of Tasmania."""

from dataprep.eda import plot
plot(merged_data['tvviewers'])

merged_data['tv_viewers/capita'] = merged_data['tvviewers']/10680976
merged_data['tv_viewers/capita']

plot(merged_data['tv_viewers/capita'])

# change y to a very small number if it's 0, because denominator cannot be 0
y = merged_data['tv_viewers/capita']
y[y==0]=0.0001

#transform tv viewers variable to log 
merged_data['tv_viewers/capita_log'] = np.log(merged_data['tv_viewers/capita'])
plot(merged_data['tv_viewers/capita_log'])

#check correlation between tv rating and other variables
from dataprep.eda import plot_correlation
plot_correlation(merged_data,'tv_viewers/capita_log')

from sklearn import datasets, linear_model
from scipy import stats

X = merged_data[['number_of_tweet','number_of_player','number_of_team','Brisbane','Melbourne','Perth','Sydney','Tasmania','total_likes','total_retweets','total_comments']]
y = merged_data['tv_viewers/capita_log']

X2 = sm.add_constant(X)
est = sm.OLS(y, X2)
est2 = est.fit()
print(est2.summary())

#OLS regression model with significant variables p-value <0.05
X = merged_data[['number_of_tweet','number_of_player','Brisbane','Melbourne','Perth','Sydney','Tasmania','total_comments']]
y = merged_data['tv_viewers/capita_log']

X2 = sm.add_constant(X)
est = sm.OLS(y, X2)
est2 = est.fit()
print(est2.summary())

"""### **Q3(2): How does eWOM communications impact online engagements on different race weekends?**"""

X = merged_data[['event_number','number_of_tweet','number_of_player','number_of_team',6,7]]
y2 = merged_data['total_likes']

X2 = sm.add_constant(X)
est = sm.OLS(y2, X2)
est2 = est.fit()
print(est2.summary())

X = merged_data[['event_number','number_of_tweet','number_of_player','number_of_team',6,7]]
y3 = merged_data['total_retweets']

X2 = sm.add_constant(X)
est = sm.OLS(y3, X2)
est2 = est.fit()
print(est2.summary())

X = merged_data[['number_of_tweet','number_of_player','number_of_team',6,7,'total_retweets','total_likes']]
y4 = merged_data['total_comments']

#check correlation between total comments and other variables
from dataprep.eda import plot_correlation
plot_correlation(merged_data,"total_comments")

X2 = sm.add_constant(X)
est = sm.OLS(y4, X2)
est2 = est.fit()
print(est2.summary())

!pip install tabulate #https://towardsdatascience.com/how-to-easily-create-tables-in-python-2eaea447d8fd
from tabulate import tabulate

table = ['Variable','R-Squared Score'], ['total_likes','0.533'],['total_retweets','0.535'],['total_comments','0.657']
print(tabulate(table,headers='firstrow', tablefmt='fancy_grid'))